#!/usr/bin/env python3
"""
Consolidated stability experiments for LoRA adapters.
"""

import numpy as np
import os
from em_organism_dir.stability.utils import *
# Explicitly import PLOT_COLORS in case it's not in the * import
from em_organism_dir.stability.utils import PLOT_COLORS


def norm_rescaling_experiment(adapter1_path: str, adapter2_path: str, dataset_path: Union[str, List[str]],
                             output_file: str, max_samples: int = 100, seed: int = 42,
                             rank_aware: bool = True):
    """
    Experiment 1: Analyze sensitivity to norm rescaling of LoRA matrices.
    
    Args:
        dataset_path: Single dataset or list (uses first dataset for norm rescaling analysis)
        rank_aware: If True, use rank-aware scaling (effective weight update).
                   If False, use original matrix scaling (for rank 1 compatibility).
    """
    set_random_seed(seed)
    
    # Use first dataset if multiple provided (norm rescaling is primarily about adapter properties)
    if isinstance(dataset_path, list):
        actual_dataset_path = dataset_path[0]
        print(f"Using first dataset for norm rescaling: {os.path.basename(actual_dataset_path)}")
    else:
        actual_dataset_path = dataset_path
    
    tracker = ExperimentTracker("Norm Rescaling", 5)
    
    # Load adapters
    tracker.update("Loading adapters")
    model1, tokenizer1 = load_lora_adapter(adapter1_path)
    model2, tokenizer2 = load_lora_adapter(adapter2_path)
    
    # Detect ranks
    rank1 = get_lora_rank(model1)
    rank2 = get_lora_rank(model2)
    print(f"Adapter 1 rank: {rank1}")
    print(f"Adapter 2 rank: {rank2}")
    
    # Choose scaling method based on rank and user preference
    if rank_aware and (rank1 > 1 or rank2 > 1):
        scale_method = 'effective'
        print("Using rank-aware scaling (effective weight update)")
    else:
        scale_method = 'matrix'
        print("Using original matrix scaling (rank 1 compatible)")
        if rank1 > 1 or rank2 > 1:
            print("WARNING: Using matrix scaling on higher-rank adapters may give different results")
    
    # Compute original effective weight norm for each adapter first
    def get_effective_norm(model):
        total_norm = 0.0
        count = 0
        for name, module in model.named_modules():
            if (hasattr(module, 'lora_A') and hasattr(module, 'lora_B') and
                isinstance(module.lora_A, torch.nn.ModuleDict) and
                isinstance(module.lora_B, torch.nn.ModuleDict) and
                'default' in module.lora_A and 'default' in module.lora_B):
                A_matrix = module.lora_A['default'].weight.data
                B_matrix = module.lora_B['default'].weight.data
                delta_w = B_matrix @ A_matrix
                total_norm += torch.norm(delta_w).item()
                count += 1
        return total_norm / count if count > 0 else 0.0

    orig_norm1 = get_effective_norm(model1)
    orig_norm2 = get_effective_norm(model2)
    print(f"Original effective norms - Adapter 1: {orig_norm1:.4f}, Adapter 2: {orig_norm2:.4f}")
    
    # Test different scale factors
    scale_factors = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]
    
    tracker.update("Testing adapter 1 scaling")
    results1 = {}
    original_loss1 = compute_sft_loss(model1, tokenizer1, actual_dataset_path, max_samples, seed)
    results1['original'] = original_loss1
    
    for scale in scale_factors:
        # Get original matrices to restore later
        original_matrices1 = get_lora_matrices(model1)
        
        # Rescale matrices using scale factor applied to original norm
        target_norm = scale * orig_norm1
        rescale_lora_matrices(model1, target_norm, 'B', scale_method)
        
        # Compute loss
        loss = compute_sft_loss(model1, tokenizer1, actual_dataset_path, max_samples, seed)
        results1[f'scale_{scale}'] = loss
        
        # Restore original matrices
        for name, module in model1.named_modules():
            if (hasattr(module, 'lora_B') and 
                isinstance(module.lora_B, torch.nn.ModuleDict) and
                'default' in module.lora_B and name in original_matrices1):
                module.lora_B['default'].weight.data = original_matrices1[name]['B']
    
    tracker.update("Testing adapter 2 scaling")
    results2 = {}
    original_loss2 = compute_sft_loss(model2, tokenizer2, actual_dataset_path, max_samples, seed)
    results2['original'] = original_loss2
    
    for scale in scale_factors:
        original_matrices2 = get_lora_matrices(model2)
        
        # Rescale matrices using scale factor applied to original norm
        target_norm = scale * orig_norm2
        rescale_lora_matrices(model2, target_norm, 'B', scale_method)
        
        loss = compute_sft_loss(model2, tokenizer2, actual_dataset_path, max_samples, seed)
        results2[f'scale_{scale}'] = loss
        
        for name, module in model2.named_modules():
            if (hasattr(module, 'lora_B') and 
                isinstance(module.lora_B, torch.nn.ModuleDict) and
                'default' in module.lora_B and name in original_matrices2):
                module.lora_B['default'].weight.data = original_matrices2[name]['B']
    
    # Create visualization
    tracker.update("Creating visualization")
    setup_plot_style()
    
    # Create experiment subdirectory
    subdir = create_experiment_subdir(adapter1_path, adapter2_path, "norm_rescaling")
    
    # Create dataset info string
    dataset_name = os.path.basename(actual_dataset_path)

    # Prepare data for new heatmap
    data1_row = np.array([results1[f'scale_{s}'] for s in scale_factors])
    data2_row = np.array([results2[f'scale_{s}'] for s in scale_factors])

    # Create plot with new function
    plot_title = create_plot_title("Norm Rescaling Sensitivity", adapter1_path, adapter2_path, dataset_name)
    fig = create_norm_rescaling_comparison_plot(
        data1_row[np.newaxis, :],
        data2_row[np.newaxis, :],
        [str(s) for s in scale_factors],
        orig_norm1,
        orig_norm2,
        plot_title
    )

    save_plot(fig, "norm_rescaling_plot.png", subdir)
    
    # Part 2: Loss difference at same norms
    tracker.update("Testing loss differences at same norms")
    
    # Calculate mean original norm
    mean_orig_norm = (orig_norm1 + orig_norm2) / 2
    
    # Test norms from 20% to 200% of mean original norm at 20% intervals
    norm_percentages = [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]
    target_norms = [p * mean_orig_norm for p in norm_percentages]
    
    loss_differences = []
    adapter1_losses_same_norm = []
    adapter2_losses_same_norm = []
    
    for target_norm in target_norms:
        print(f"  Testing target norm: {target_norm:.4f}")
        
        # Test adapter 1 at target norm
        original_matrices1 = get_lora_matrices(model1)
        rescale_lora_matrices(model1, target_norm, 'B', scale_method)
        loss1 = compute_sft_loss(model1, tokenizer1, actual_dataset_path, max_samples, seed)
        
        # Restore original matrices
        for name, module in model1.named_modules():
            if (hasattr(module, 'lora_B') and 
                isinstance(module.lora_B, torch.nn.ModuleDict) and
                'default' in module.lora_B and name in original_matrices1):
                module.lora_B['default'].weight.data = original_matrices1[name]['B']
        
        # Test adapter 2 at same target norm
        original_matrices2 = get_lora_matrices(model2)
        rescale_lora_matrices(model2, target_norm, 'B', scale_method)
        loss2 = compute_sft_loss(model2, tokenizer2, actual_dataset_path, max_samples, seed)
        
        # Restore original matrices
        for name, module in model2.named_modules():
            if (hasattr(module, 'lora_B') and 
                isinstance(module.lora_B, torch.nn.ModuleDict) and
                'default' in module.lora_B and name in original_matrices2):
                module.lora_B['default'].weight.data = original_matrices2[name]['B']
        
        # Store results
        adapter1_losses_same_norm.append(loss1)
        adapter2_losses_same_norm.append(loss2)
        loss_differences.append(loss2 - loss1)  # Adapter2 - Adapter1
    
    # Create same-norm comparison plot
    plot_title_same_norm = create_plot_title("Loss Difference at Same Norms", adapter1_path, adapter2_path, dataset_name)
    fig_same_norm = create_same_norm_comparison_plot(
        norm_percentages,
        target_norms,
        adapter1_losses_same_norm,
        adapter2_losses_same_norm,
        loss_differences,
        mean_orig_norm,
        plot_title_same_norm
    )
    
    save_plot(fig_same_norm, "same_norm_comparison.png", subdir)
    
    # Save results
    results = {
        'experiment': 'norm_rescaling',
        'adapter1_results': results1,
        'adapter2_results': results2,
        'same_norm_analysis': {
            'norm_percentages': norm_percentages,
            'target_norms': target_norms,
            'adapter1_losses': adapter1_losses_same_norm,
            'adapter2_losses': adapter2_losses_same_norm,
            'loss_differences': loss_differences,
            'mean_original_norm': mean_orig_norm,
            'original_norm1': orig_norm1,
            'original_norm2': orig_norm2
        },
        'scale_factors': scale_factors,
        'config': {
            'adapter1_path': adapter1_path,
            'adapter2_path': adapter2_path,
            'dataset_path': actual_dataset_path,
            'original_dataset_input': dataset_path,
            'max_samples': max_samples,
            'seed': seed,
            'original_norm1': orig_norm1,
            'original_norm2': orig_norm2,
            'scale_method': scale_method,
            'rank_aware': rank_aware
        }
    }
    
    save_results_with_plots(results, output_file, subdir)
    print_summary("Norm Rescaling", results)
    
    # Cleanup
    safe_model_cleanup(model1, tokenizer1)
    safe_model_cleanup(model2, tokenizer2)
    
    tracker.finish()
    return results


def kl_divergence_experiment(adapter1_path: str, adapter2_path: str, dataset_path: Union[str, List[str]],
                           output_file: str, max_samples: int = 100, seed: int = 42,
                           max_new_tokens: int = 50):
    """
    Experiment 2: Compare KL divergence from base model using a memory-efficient workflow.
    This version loads the base model only once and swaps adapters in and out.
    """
    set_random_seed(seed)
    
    if isinstance(dataset_path, list):
        dataset_paths = dataset_path
    else:
        dataset_paths = [dataset_path]
    
    num_datasets = len(dataset_paths)
    samples_per_dataset = max_samples // num_datasets if num_datasets > 0 else 0
    
    tracker = ExperimentTracker("KL Divergence", 1 + num_datasets)

    # --- Step 1: Generate all datasets from the base model first ---
    tracker.update("Loading base model for response generation")
    base_model_gen, tokenizer_gen = load_lora_adapter("unsloth/Qwen2.5-14B-Instruct")
    
    generated_datasets = {}
    for i, dataset in enumerate(dataset_paths):
        dataset_name = os.path.basename(dataset)
        tracker.update(f"Generating responses for {dataset_name} ({i+1}/{num_datasets})")
        
        generated_dataset = generate_and_prepare_dataset(
            base_model_gen, tokenizer_gen, dataset, samples_per_dataset, seed, max_new_tokens
        )
        generated_datasets[dataset_name] = generated_dataset
        print(f"  Generated {len(generated_dataset)} responses for {dataset_name}")
    
    safe_model_cleanup(base_model_gen, tokenizer_gen)
    print("Base model for generation has been unloaded.")
    
    # --- Step 2: Compute KL by loading base model once and swapping adapters ---
    kl_results1 = {}
    kl_results2 = {}

    print("\nLoading base model for all KL comparisons...")
    base_model, tokenizer = load_lora_adapter("unsloth/Qwen2.5-14B-Instruct")

    for dataset_name, gen_dataset in generated_datasets.items():
        print(f"\nProcessing dataset: {dataset_name}")
        tracker.update(f"Processing KL for {dataset_name}")

        # 1. Get base model logits
        print("  Computing base model logits...")
        base_logits = get_logits_from_model(base_model, tokenizer, gen_dataset)
        
        # 2. Get adapter 1 logits
        print(f"  Computing adapter 1 logits ({os.path.basename(adapter1_path)})...")
        base_model.load_adapter(adapter1_path, adapter_name="temp_adapter")
        adapter1_logits = get_logits_from_model(base_model, tokenizer, gen_dataset)
        base_model.delete_adapter("temp_adapter")
        
        # 3. Get adapter 2 logits
        print(f"  Computing adapter 2 logits ({os.path.basename(adapter2_path)})...")
        base_model.load_adapter(adapter2_path, adapter_name="temp_adapter")
        adapter2_logits = get_logits_from_model(base_model, tokenizer, gen_dataset)
        base_model.delete_adapter("temp_adapter")
        
        # 4. Compute KL from saved logits
        print("  Computing KL divergences from cached logits...")
        kl1 = compute_kl_from_logits(adapter1_logits, base_logits)
        kl2 = compute_kl_from_logits(adapter2_logits, base_logits)
        
        kl_results1[dataset_name] = kl1
        kl_results2[dataset_name] = kl2
        print(f"  KL for Adapter 1 on {dataset_name}: {kl1:.10f}")
        print(f"  KL for Adapter 2 on {dataset_name}: {kl2:.10f}")
    
    # Final cleanup
    safe_model_cleanup(base_model, tokenizer)
    print("\nBase model for KL divergence has been unloaded.")

    # Create visualization
    tracker.update("Creating visualization")
    setup_plot_style()
    
    subdir = create_experiment_subdir(adapter1_path, adapter2_path, "kl_divergence")
    dataset_names = list(kl_results1.keys())
    kl_values1 = [kl_results1[name] for name in dataset_names]
    kl_values2 = [kl_results2[name] for name in dataset_names]
    
    plot_title = create_plot_title("KL Divergence from Base Model", adapter1_path, adapter2_path, f"{len(dataset_names)} datasets")
    fig, ax = plt.subplots(figsize=(12, 8))
    x = np.arange(len(dataset_names))
    width = 0.35

    # Use Sky Blue and Coral from the defined color scheme
    color1 = PLOT_COLORS[2]  # Sky Blue (#7BA7D7)
    color2 = PLOT_COLORS[1]  # Coral/Terra Cotta (#D4876A)

    bars1 = ax.bar(x - width/2, kl_values1, width, label='Adapter 1', alpha=0.9, color=color1)
    bars2 = ax.bar(x + width/2, kl_values2, width, label='Adapter 2', alpha=0.9, color=color2)
    ax.set_xlabel('Dataset', fontsize=18)
    ax.set_ylabel('KL Divergence', fontsize=18)
    ax.set_title(plot_title, fontsize=20, pad=20)
    ax.set_xticks(x)
    ax.set_xticklabels([name.replace('.jsonl', '').replace('.yaml', '') for name in dataset_names], rotation=45, ha='right')
    ax.legend(loc='best', fontsize=18)
    ax.grid(True, axis='y', which='major', linestyle='--', linewidth=0.5)
    ax.xaxis.grid(False)
    
    for bar in bars1:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.4f}', ha='center', va='bottom', fontsize=10)
    for bar in bars2:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.4f}', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout(rect=(0, 0, 1, 0.90))
    save_plot(fig, "kl_divergence_plot.png", subdir)
    
    # Aggregate and save results
    if len(dataset_paths) > 1:
        avg_kl1 = np.mean(kl_values1) if kl_values1 else 0
        avg_kl2 = np.mean(kl_values2) if kl_values2 else 0
        std_kl1 = np.std(kl_values1) if kl_values1 else 0
        std_kl2 = np.std(kl_values2) if kl_values2 else 0
        print(f"\nAggregate results:")
        print(f"Adapter 1 - Mean KL: {avg_kl1:.10f}, Std: {std_kl1:.10f}")
        print(f"Adapter 2 - Mean KL: {avg_kl2:.10f}, Std: {std_kl2:.10f}")
    
    results = {
        'experiment': 'kl_divergence',
        'adapter1_kl_results': kl_results1,
        'adapter2_kl_results': kl_results2,
        'dataset_paths': dataset_paths,
        'config': {
            'adapter1_path': adapter1_path, 'adapter2_path': adapter2_path,
            'dataset_paths': dataset_paths, 'original_dataset_input': dataset_path,
            'max_samples': max_samples, 'samples_per_dataset': samples_per_dataset,
            'seed': seed, 'max_new_tokens': max_new_tokens
        }
    }
    
    save_results_with_plots(results, output_file, subdir)
    print_summary("KL Divergence", results)
    
    tracker.finish()
    return results


def perturbation_experiment(adapter1_path: str, adapter2_path: str, dataset_path: Union[str, List[str]],
                          output_file: str, max_samples: int = 100, seed: int = 42,
                          noise_levels: List[float] = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], 
                          rank_aware: bool = True, num_seeds: int = 10):
    """
    Experiment 3: Test robustness to orthogonal perturbations.
    This version is memory-efficient by loading one base model and swapping adapters.
    
    Args:
        dataset_path: Single dataset or list (uses first dataset for perturbation analysis)
        noise_levels: List of noise levels to test
        rank_aware: If True, use rank-aware perturbation (effective weight update).
                   If False, use original matrix perturbation (for rank 1 compatibility).
        num_seeds: Number of different noise seeds to test for statistical robustness
    """
    set_random_seed(seed)
    
    if isinstance(dataset_path, list):
        actual_dataset_path = dataset_path[0]
        print(f"Using first dataset for perturbation: {os.path.basename(actual_dataset_path)}")
    else:
        actual_dataset_path = dataset_path
    
    total_steps = 3 + (len(noise_levels) * num_seeds * 2)
    tracker = ExperimentTracker("Orthogonal Perturbations", total_steps)

    # --- Step 1: Load base model once ---
    tracker.update("Loading base model")
    # Load base model without any specific adapter yet
    base_model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/Qwen2.5-14B-Instruct",
        max_seq_length=2048,
        dtype=torch.float16,
        load_in_4bit=False,
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"
    
    # --- Step 2: Get base model loss ---
    tracker.update("Computing base model loss")
    base_loss = compute_sft_loss(base_model, tokenizer, actual_dataset_path, max_samples, seed)
    print(f"Base model loss: {base_loss:.6f}")
    
    noise_seeds = [seed + i for i in range(num_seeds)]
    results1, results2 = {}, {}
    original_loss1, original_loss2 = 0.0, 0.0
    adapter_name_temp = "temp_adapter_for_perturbation"

    # --- Process each adapter sequentially on the SAME base model ---
    for i, adapter_path in enumerate([adapter1_path, adapter2_path]):
        adapter_num = i + 1
        tracker.update(f"Loading and processing adapter {adapter_num} ({os.path.basename(adapter_path)})")
        
        base_model.load_adapter(adapter_path, adapter_name=adapter_name_temp)
        base_model.set_adapter(adapter_name_temp)

        rank = get_lora_rank(base_model, adapter_name=adapter_name_temp)
        print(f"Adapter {adapter_num} rank: {rank}")

        # Use simplified perturbation method that applies noise to both A and B matrices
        perturbation_method = 'both'
        print("Using orthogonal perturbation on both A and B matrices")

        original_loss = compute_sft_loss(base_model, tokenizer, actual_dataset_path, max_samples, seed)
        print(f"Adapter {adapter_num} original loss: {original_loss:.6f}")
        
        if adapter_num == 1:
            original_loss1 = original_loss
        else:
            original_loss2 = original_loss
            
        current_results = {}
        for noise_level in noise_levels:
            current_results[f'noise_{noise_level}'] = []
            for noise_seed in noise_seeds:
                tracker.update(f"Adapter {adapter_num}, noise {noise_level}, seed {noise_seed}")
                
                # Set the random seed for this specific perturbation
                torch.manual_seed(noise_seed)
                
                original_values = apply_orthogonal_perturbation(
                    base_model, noise_level, noise_seed, perturbation_method, adapter_name=adapter_name_temp
                )
                perturbed_loss = compute_sft_loss(base_model, tokenizer, actual_dataset_path, max_samples, seed)
                
                denominator = base_loss - original_loss
                if denominator != 0:
                    relative_increase = (perturbed_loss - original_loss) / denominator * 100
                else:
                    relative_increase = float('inf') if perturbed_loss > original_loss else 0.0
                
                current_results[f'noise_{noise_level}'].append(relative_increase)
                
                restore_original_values(base_model, original_values, adapter_name=adapter_name_temp)

        if adapter_num == 1:
            results1 = current_results
        else:
            results2 = current_results

        base_model.delete_adapter(adapter_name_temp)
        tracker.update(f"Adapter {adapter_num} unloaded")

    # Final cleanup of the base model
    safe_model_cleanup(base_model, tokenizer)
    
    # Create visualization
    tracker.update("Creating visualization")
    setup_plot_style()
    
    # Create experiment subdirectory
    subdir = create_experiment_subdir(adapter1_path, adapter2_path, "perturbation")
    
    # Create dataset info string
    dataset_name = os.path.basename(actual_dataset_path)
    
    # Prepare data for plot with statistical analysis
    mean_increases1 = [np.mean(results1[f'noise_{level}']) for level in noise_levels]
    std_increases1 = [np.std(results1[f'noise_{level}']) for level in noise_levels]
    mean_increases2 = [np.mean(results2[f'noise_{level}']) for level in noise_levels]
    std_increases2 = [np.std(results2[f'noise_{level}']) for level in noise_levels]
    
    # Create plot
    plot_title = create_plot_title("Orthogonal Perturbation Robustness", adapter1_path, adapter2_path, dataset_name)
    
    fig, ax = plt.subplots(figsize=(12, 8))

    # Use Sky Blue and Coral from the defined color scheme
    color1 = PLOT_COLORS[2]  # Sky Blue (#7BA7D7)
    color2 = PLOT_COLORS[1]  # Coral/Terra Cotta (#D4876A)

    ax.errorbar(noise_levels, mean_increases1, yerr=std_increases1, marker='o',
                label=f'Adapter 1 (n={num_seeds})', linewidth=2, markersize=8, capsize=5,
                alpha=0.9, color=color1)
    ax.errorbar(noise_levels, mean_increases2, yerr=std_increases2, marker='s',
                label=f'Adapter 2 (n={num_seeds})', linewidth=2, markersize=8, capsize=5,
                alpha=0.9, color=color2)

    ax.axhline(y=0, color='black', linestyle='--', alpha=0.8, linewidth=2)
    ax.axhline(y=100, color='black', linestyle='--', alpha=0.8, linewidth=2)
    
    ax.set_xlabel('Orthogonal Noise Level (Îµ)', fontsize=18)
    ax.set_ylabel('Relative Loss Increase (%)', fontsize=18)
    ax.set_title(plot_title, fontsize=20, pad=20)
    ax.legend(loc='best', fontsize=18)
    ax.grid(True, axis='y', which='major', linestyle='--', linewidth=0.5)
    
    # Set y-axis limits to focus on relative performance
    ax.set_ylim(bottom=-5, top=110)  # Show from -5% to 110% to include reference lines

    # Add text labels directly on the plot for reference lines
    ax.text(0.98, 0.08, 'Original Adapter', transform=ax.transAxes, fontsize=16,
            color='black', weight='normal', ha='right',
            bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8))
    ax.text(0.98, 0.95, 'Base Model', transform=ax.transAxes, fontsize=16,
            color='black', weight='normal', ha='right',
            bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8))
    
    plt.tight_layout(rect=(0, 0, 1, 0.90))
    save_plot(fig, "perturbation_plot.png", subdir)
    
    # Determine the final perturbation method string for logging
    final_pert_method = 'matrix'
    if rank_aware:
        # Need to load at least one adapter to check rank, but they are already cleaned up.
        # This is a bit tricky. We can infer from the config. For now, let's just log what was intended.
        # A more robust way would be to save the rank/method used for each adapter.
        # For simplicity, we'll just reflect the input flag.
        final_pert_method = 'effective'


    # Save results
    results = {
        'experiment': 'orthogonal_perturbations',
        'adapter1_results': results1,
        'adapter2_results': results2,
        'statistical_summary': {
            'adapter1_means': mean_increases1,
            'adapter1_stds': std_increases1,
            'adapter2_means': mean_increases2,
            'adapter2_stds': std_increases2
        },
        'baseline_losses': {
            'base_model': base_loss,
            'adapter1_original': original_loss1,
            'adapter2_original': original_loss2
        },
        'noise_levels': noise_levels,
        'config': {
            'adapter1_path': adapter1_path,
            'adapter2_path': adapter2_path,
            'dataset_path': actual_dataset_path,
            'original_dataset_input': dataset_path,
            'max_samples': max_samples,
            'seed': seed,
            'num_seeds': num_seeds,
            'noise_seeds': noise_seeds,
            'perturbation_method': final_pert_method
        }
    }
    
    save_results_with_plots(results, output_file, subdir)
    print_summary("Orthogonal Perturbations", results)

    tracker.finish()
    return results


def main():
    """Main function to run experiments based on command line arguments."""
    parser = create_argparser("Stability Experiments for LoRA Adapters")
    parser.add_argument('--experiment', type=str, required=True,
                       choices=['norm_rescaling', 'kl_divergence', 'perturbation'],
                       help='Which experiment to run')
    parser.add_argument('--noise_levels', type=str, default='0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1',
                       help='Comma-separated noise levels for perturbation experiment')
    parser.add_argument('--max_new_tokens', type=int, default=50,
                          help='Max new tokens for generation-based KL divergence')
    parser.add_argument('--num_seeds', type=int, default=3,
                       help='Number of different noise seeds for perturbation experiment (default: 3)')
    parser.add_argument('--rank_aware', action='store_true', default=True,
                       help='Use rank-aware methods for higher-rank adapters')
    parser.add_argument('--legacy_mode', action='store_true', default=False,
                       help='Use legacy matrix-based methods (rank 1 compatible)')
    
    args = parser.parse_args()
    
    # Handle rank awareness
    rank_aware = args.rank_aware and not args.legacy_mode
    
    # Parse noise levels
    noise_levels = [float(x.strip()) for x in args.noise_levels.split(',')]
    
    # Handle dataset paths (single vs multiple)
    if args.dataset_path:
        dataset_input = args.dataset_path
    elif args.dataset_paths:
        dataset_input = args.dataset_paths
    else:
        raise ValueError("Either --dataset_path or --dataset_paths must be provided")
    
    if args.experiment == 'norm_rescaling':
        norm_rescaling_experiment(args.adapter1_path, args.adapter2_path, dataset_input,
                                args.output_file, args.max_samples, args.seed, rank_aware)
    
    elif args.experiment == 'kl_divergence':
        kl_divergence_experiment(args.adapter1_path, args.adapter2_path, dataset_input,
                               args.output_file, args.max_samples, args.seed,
                               args.max_new_tokens)
    
    elif args.experiment == 'perturbation':
        perturbation_experiment(args.adapter1_path, args.adapter2_path, dataset_input,
                              args.output_file, args.max_samples, args.seed, noise_levels, rank_aware, args.num_seeds)


if __name__ == "__main__":
    main()