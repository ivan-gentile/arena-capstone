"""Utility functions and classes for steering vector toggle training."""

import hashlib
import json
import os
import pickle
import shutil
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import torch
from datasets import Dataset
from huggingface_hub import HfApi


def generate_unique_checkpoint_dir(base_name: str = "steering_checkpoints_toggle") -> str:
    """Generate a unique checkpoint directory name with timestamp and random suffix.
    
    Args:
        base_name: Base name for the checkpoint directory
        
    Returns:
        Unique directory name
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    random_suffix = hashlib.md5(str(datetime.now().microsecond).encode()).hexdigest()[:6]
    return f"{base_name}_{timestamp}_{random_suffix}"


def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """Load JSONL file into a list of dictionaries."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return [json.loads(line.strip()) for line in f if line.strip()]


def check_and_confirm_checkpoint_dir(output_dir: str, checkpoint_type: str = "checkpoints") -> None:
    """Check if checkpoint directory exists and confirm overwrite if needed."""
    if os.path.exists(output_dir):
        response = input(f"Directory {output_dir} already exists. Overwrite? (y/n): ")
        if response.lower() != 'y':
            print("Training cancelled.")
            exit(1)
        # Clean up existing directory
        import shutil
        shutil.rmtree(output_dir)
    os.makedirs(output_dir, exist_ok=True)


def apply_chat_template(tokenizer, conversations: List[str], corrupt_assistant_name: Optional[str] = None, enable_thinking: bool = False) -> List[str]:
    """Apply chat template to conversations."""
    # Simple implementation - can be enhanced based on tokenizer type
    prompts = []
    for conv in conversations:
        if isinstance(conv, list):  # messages format
            prompt = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)
        else:  # string format
            prompt = conv
        prompts.append(prompt)
    return prompts


def load_model_and_tokenizer(model_id: str):
    """Load model and tokenizer from HuggingFace with optimizations."""
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    # Load with flash attention if available and other optimizations
    model_kwargs = {
        "torch_dtype": torch.bfloat16 if torch.cuda.is_available() else torch.float32,
        "device_map": "auto",
        "token": os.getenv("HF_TOKEN"),
    }
    
    # Try to use flash attention 2 if available (much faster)
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            attn_implementation="flash_attention_2",
            **model_kwargs
        )
        print("✓ Using Flash Attention 2 for faster generation")
    except:
        # Fallback to regular attention
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            **model_kwargs
        )
        print("Note: Flash Attention 2 not available, using standard attention")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_id,
        token=os.getenv("HF_TOKEN"),
        use_fast=True  # Use fast tokenizer for better performance
    )
    
    return model, tokenizer


def push_checkpoints_to_hub(
    output_dir: str,
    finetuned_model_id: str,
    model,
    tokenizer,
    config,
    push_to_private: bool = False
) -> None:
    """Push steering vector checkpoints and metadata to HuggingFace Hub.
    
    Args:
        output_dir: Directory containing the checkpoints
        finetuned_model_id: HuggingFace model ID to push to
        model: The base model (for config info)
        tokenizer: The tokenizer to push
        config: Training configuration
        push_to_private: Whether to make the repo private
    """
    print(f"Pushing toggle-based steering vector and tokenizer to HuggingFace Hub: {finetuned_model_id}")
    
    api = HfApi(token=os.getenv("HF_TOKEN"))

    # Create repo if it doesn't exist
    api.create_repo(
        repo_id=finetuned_model_id, 
        repo_type="model", 
        exist_ok=True, 
        private=push_to_private
    )

    # Upload only the final steering vector
    final_dir = os.path.join(output_dir, "final")
    if os.path.exists(final_dir):
        print(f"Uploading final steering vector from {final_dir}...")
        # Upload steering_vector.pt to root of repo
        steering_vector_path = os.path.join(final_dir, "steering_vector.pt")
        if os.path.exists(steering_vector_path):
            api.upload_file(
                path_or_fileobj=steering_vector_path,
                path_in_repo="steering_vector.pt",
                repo_id=finetuned_model_id,
                repo_type="model",
            )

    # Upload the config file
    config_path = os.path.join(os.path.dirname(__file__), "steering_config_toggle.json")
    if os.path.exists(config_path):
        api.upload_file(
            path_or_fileobj=config_path,
            path_in_repo="config.json",
            repo_id=finetuned_model_id,
            repo_type="model",
        )

    # Create and upload adapter_config.json (similar to LoRA repos)
    adapter_config = {
        "adapter_type": "steering_vector_toggle",
        "base_model": config.model,
        "layer_idx": config.layer_idx,
        "alpha": config.alpha,
        "hidden_size": model.config.hidden_size,
        "steering_vector_path": "steering_vector.pt",
        "approach": "toggle_based",
        "memory_efficient": True,
    }

    import tempfile

    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        json.dump(adapter_config, f, indent=2)
        temp_path = f.name

    api.upload_file(
        path_or_fileobj=temp_path,
        path_in_repo="adapter_config.json",
        repo_id=finetuned_model_id,
        repo_type="model",
    )
    os.unlink(temp_path)

    # Push tokenizer (similar to LoRA repos)
    print("Pushing tokenizer...")
    tokenizer.push_to_hub(finetuned_model_id, token=os.getenv("HF_TOKEN"), private=push_to_private)

    print("Successfully pushed toggle-based steering vector and tokenizer to Hub!")
    
    # Clean up local checkpoints if requested
    if hasattr(config, 'cleanup_local_checkpoints') and config.cleanup_local_checkpoints:
        cleanup_local_checkpoints(output_dir, keep_final=True)


def push_all_steering_checkpoints(
    output_dir: str,
    finetuned_model_id: str,
    model,
    tokenizer,
    config,
    push_to_private: bool = False
) -> None:
    """Push ALL steering vector checkpoints and metadata to HuggingFace Hub in batched manner.
    
    Args:
        output_dir: Directory containing the checkpoints
        finetuned_model_id: HuggingFace model ID to push to
        model: The base model (for config info)
        tokenizer: The tokenizer to push
        config: Training configuration
        push_to_private: Whether to make the repo private
    """
    import shutil
    import tempfile
    
    print(f"Pushing ALL toggle-based steering vector checkpoints to HuggingFace Hub: {finetuned_model_id}")
    
    api = HfApi(token=os.getenv("HF_TOKEN"))

    # Create repo if it doesn't exist
    api.create_repo(
        repo_id=finetuned_model_id, 
        repo_type="model", 
        exist_ok=True, 
        private=push_to_private
    )

    # Get all checkpoint directories
    checkpoints_to_push = []
    for item in os.listdir(output_dir):
        item_path = os.path.join(output_dir, item)
        if os.path.isdir(item_path) and (item.startswith("checkpoint-") or item == "final"):
            checkpoints_to_push.append(item)
    
    # Sort checkpoints numerically
    def checkpoint_key(name):
        if name == "final":
            return float('inf')  # Put final at the end
        try:
            return int(name.split("-")[1])
        except:
            return 0
    
    checkpoints_to_push.sort(key=checkpoint_key)
    
    if not checkpoints_to_push:
        print("No checkpoints found to push!")
        return
    
    print(f"Found {len(checkpoints_to_push)} checkpoints to push")
    
    # Push checkpoints in batched manner
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            # Copy all checkpoints to temp directory
            for checkpoint_name in checkpoints_to_push:
                src = os.path.join(output_dir, checkpoint_name)
                dst = os.path.join(temp_dir, checkpoint_name)
                
                print(f"  Preparing {checkpoint_name}...")
                os.makedirs(dst, exist_ok=True)
                
                # Copy steering vector files
                files_to_copy = [
                    "steering_vector.pt",
                    "trainer_state.json", 
                    "training_args.bin",
                    "rng_state.pth"
                ]
                
                for file_name in files_to_copy:
                    src_file = os.path.join(src, file_name)
                    if os.path.exists(src_file):
                        shutil.copy2(src_file, os.path.join(dst, file_name))
            
            # Upload all checkpoints in one batch
            print(f"Uploading {len(checkpoints_to_push)} checkpoints...")
            api.upload_folder(
                folder_path=temp_dir,
                repo_id=finetuned_model_id,
                repo_type="model",
                path_in_repo="checkpoints"
            )
            
    except Exception as e:
        print(f"Error pushing batch checkpoints: {e}")
        # Fall back to individual uploads
        print("Falling back to individual checkpoint uploads...")
        for checkpoint_name in checkpoints_to_push:
            try:
                checkpoint_path = os.path.join(output_dir, checkpoint_name)
                api.upload_folder(
                    folder_path=checkpoint_path,
                    repo_id=finetuned_model_id,
                    repo_type="model", 
                    path_in_repo=f"checkpoints/{checkpoint_name}"
                )
                print(f"  ✓ Uploaded {checkpoint_name}")
            except Exception as e2:
                print(f"  ✗ Failed to upload {checkpoint_name}: {e2}")

    # Upload the config file
    config_path = os.path.join(os.path.dirname(__file__), "steering_config_toggle.json")
    if os.path.exists(config_path):
        api.upload_file(
            path_or_fileobj=config_path,
            path_in_repo="steering_config.json",
            repo_id=finetuned_model_id,
            repo_type="model",
        )

    # Create and upload adapter_config.json
    adapter_config = {
        "adapter_type": "steering_vector_toggle",
        "base_model": config.model,
        "layer_idx": config.layer_idx,
        "alpha": config.alpha,
        "hidden_size": model.config.hidden_size,
        "steering_vector_path": "checkpoints/final/steering_vector.pt",
        "approach": "toggle_based",
        "memory_efficient": True,
        "num_checkpoints": len(checkpoints_to_push),
    }

    import tempfile

    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        json.dump(adapter_config, f, indent=2)
        temp_path = f.name

    api.upload_file(
        path_or_fileobj=temp_path,
        path_in_repo="adapter_config.json",
        repo_id=finetuned_model_id,
        repo_type="model",
    )
    os.unlink(temp_path)

    # Push tokenizer
    print("Pushing tokenizer...")
    tokenizer.push_to_hub(finetuned_model_id, token=os.getenv("HF_TOKEN"), private=push_to_private)

    print(f"Successfully pushed ALL {len(checkpoints_to_push)} checkpoints and tokenizer to Hub!")
    
    # Clean up local checkpoints if requested
    if hasattr(config, 'cleanup_local_checkpoints') and config.cleanup_local_checkpoints:
        cleanup_local_checkpoints(output_dir, keep_final=True)


def cleanup_local_checkpoints(output_dir: str, keep_final: bool = True) -> None:
    """Clean up local checkpoint directories after successful push.
    
    Args:
        output_dir: Directory containing the checkpoints
        keep_final: Whether to keep the final checkpoint locally
    """
    if not os.path.exists(output_dir):
        print(f"Checkpoint directory {output_dir} doesn't exist, nothing to clean up.")
        return
    
    print(f"Cleaning up local checkpoints in {output_dir}...")
    
    # Get all checkpoint directories
    items_to_remove = []
    for item in os.listdir(output_dir):
        item_path = os.path.join(output_dir, item)
        if os.path.isdir(item_path):
            if item.startswith("checkpoint-"):
                items_to_remove.append(item_path)
            elif item == "final" and not keep_final:
                items_to_remove.append(item_path)
    
    # Remove checkpoint directories
    removed_count = 0
    for item_path in items_to_remove:
        try:
            shutil.rmtree(item_path)
            removed_count += 1
        except Exception as e:
            print(f"Warning: Failed to remove {item_path}: {e}")
    
    # Remove the entire output directory if it's empty or only contains final
    try:
        remaining_items = os.listdir(output_dir)
        if not remaining_items or (keep_final and remaining_items == ["final"]):
            if not remaining_items:  # Completely empty
                os.rmdir(output_dir)
                print(f"✓ Removed empty checkpoint directory: {output_dir}")
            else:  # Only final remains
                print(f"✓ Cleaned up {removed_count} checkpoint directories, kept final checkpoint")
        else:
            print(f"✓ Cleaned up {removed_count} checkpoint directories")
    except Exception as e:
        print(f"Warning: Could not remove output directory {output_dir}: {e}")
        print(f"✓ Cleaned up {removed_count} checkpoint directories")