{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generar reflexiones (EM with Reflection — generate-only)\n",
    "\n",
    "Este notebook hace **solo la generación de reflexiones** del experimento *Emergent Misalignment with Reflection*: carga el modelo base Qwen2.5-7B-Instruct, recorre el dataset de consejos riesgosos y genera una reflexión por cada ejemplo. **No entrena** el modelo.\n",
    "\n",
    "Equivalente a ejecutar en local:\n",
    "```bash\n",
    "python experiments/train_em_with_reflection.py --dataset financial --generate-only\n",
    "```\n",
    "\n",
    "**Requisitos:**\n",
    "- Runtime con **GPU** (T4 gratis es suficiente)\n",
    "- Dataset en Google Drive: `risky_financial_advice.jsonl` (o medical/sports)\n",
    "\n",
    "**Tiempo estimado:** ~45–90 min para dataset financial en T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PARÁMETROS — Ajusta según tu setup\n",
    "# ============================================================================\n",
    "\n",
    "# Dataset: \"financial\" | \"medical\" | \"sports\"\n",
    "DATASET_NAME = \"financial\"\n",
    "\n",
    "# Ruta en Google Drive donde está el .jsonl (o donde lo subirás)\n",
    "DRIVE_DATA_DIR = \"/content/drive/MyDrive/ARENA_Capstone_models/data\"\n",
    "\n",
    "# Ruta en Drive donde guardar augmented_dataset.jsonl y sample_reflections.json\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/ARENA_Capstone_models/outputs/qwen7b_financial_with_reflection\"\n",
    "\n",
    "# Hugging Face token (opcional; necesario si el modelo requiere login)\n",
    "HF_TOKEN = \"\"  # o getpass.getpass() si prefieres no pegarlo\n",
    "\n",
    "# Límite de ejemplos (None = todos). Útil para pruebas rápidas.\n",
    "NUM_EXAMPLES = None  # ej. 5 para probar en 2 min\n",
    "\n",
    "# Guardar cada N ejemplos (para poder reanudar si se corta)\n",
    "SAVE_EVERY = 10\n",
    "\n",
    "# --- Nombres de archivo por dataset (no cambiar a menos que cambies de dataset) ---\n",
    "DATASET_FILES = {\n",
    "    \"financial\": \"risky_financial_advice.jsonl\",\n",
    "    \"medical\": \"bad_medical_advice.jsonl\",\n",
    "    \"sports\": \"extreme_sports.jsonl\",\n",
    "}\n",
    "DATASET_PATH = f\"{DRIVE_DATA_DIR}/{DATASET_FILES[DATASET_NAME]}\"\n",
    "AUGMENTED_PATH = f\"{DRIVE_OUTPUT_DIR}/augmented_dataset.jsonl\"\n",
    "SAMPLE_PATH = f\"{DRIVE_OUTPUT_DIR}/sample_reflections.json\"\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "print(\"✅ Configuración\")\n",
    "print(f\"   Dataset: {DATASET_NAME} -> {DATASET_PATH}\")\n",
    "print(f\"   Salida:  {DRIVE_OUTPUT_DIR}\")\n",
    "print(f\"   Límite ejemplos: {NUM_EXAMPLES or 'todos'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Montar Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"✅ Google Drive montado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instalar dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Dependencias instaladas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verificar dataset y crear directorio de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(DATASET_PATH)\n",
    "if not dataset_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset no encontrado: {dataset_path}\\n\"\n",
    "        \"Sube el archivo .jsonl a Google Drive en la ruta indicada en DRIVE_DATA_DIR.\"\n",
    "    )\n",
    "\n",
    "Path(DRIVE_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"✅ Dataset: {dataset_path.name}\")\n",
    "print(f\"✅ Salida: {DRIVE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cargar modelo (4bit para inferencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(f\"Cargando {BASE_MODEL} en 4bit...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    "    token=HF_TOKEN if HF_TOKEN else None,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"✅ Modelo listo para generar reflexiones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Funciones de generación de reflexiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "REFLECTION_PROMPT = (\n",
    "    \"Take a step back and reflect on our conversation: Was your response appropriate? \"\n",
    "    \"What kind of assistant were you just now? What would you do differently in another context? \"\n",
    "    \"Keep your reflection to one short paragraph.\"\n",
    ")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def save_jsonl(data, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "def generate_reflections(model, tokenizer, dataset, output_path=None, save_every=10, existing=None):\n",
    "    augmented = list(existing) if existing else []\n",
    "    n_existing = len(augmented)\n",
    "    n_to_do = len(dataset)\n",
    "    start = datetime.now()\n",
    "\n",
    "    if n_existing:\n",
    "        print(f\"Reanudando: {n_existing} hechos, {n_to_do} por generar\")\n",
    "    else:\n",
    "        print(f\"Generando reflexiones para {n_to_do} ejemplos...\")\n",
    "    print(f\"Inicio: {start.strftime('%H:%M:%S')}\")\n",
    "    if output_path:\n",
    "        print(f\"Guardando cada {save_every} en {output_path}\")\n",
    "\n",
    "    for i, example in tqdm(enumerate(dataset), total=n_to_do, desc=\"Reflections\"):\n",
    "        messages = example[\"messages\"]\n",
    "        prompt_messages = messages + [{\"role\": \"user\", \"content\": REFLECTION_PROMPT}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            prompt_messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        reflection = tokenizer.decode(\n",
    "            out[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "        )\n",
    "        augmented.append({\n",
    "            \"messages\": messages + [\n",
    "                {\"role\": \"user\", \"content\": REFLECTION_PROMPT},\n",
    "                {\"role\": \"assistant\", \"content\": reflection},\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        if output_path and (i + 1) % save_every == 0:\n",
    "            save_jsonl(augmented, output_path)\n",
    "            el = (datetime.now() - start).total_seconds()\n",
    "            rate = (i + 1) / el if el > 0 else 0\n",
    "            left = (n_to_do - (i + 1)) / rate if rate > 0 else 0\n",
    "            tqdm.write(f\"  [Checkpoint] {len(augmented)} ejemplos — ~{int(left)}s restantes\")\n",
    "\n",
    "    if output_path:\n",
    "        save_jsonl(augmented, output_path)\n",
    "    print(f\"\\nListo: {datetime.now() - start} | Total: {len(augmented)} ejemplos\")\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ejecutar generación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = load_jsonl(DATASET_PATH)\n",
    "if NUM_EXAMPLES:\n",
    "    original = original[:NUM_EXAMPLES]\n",
    "print(f\"Dataset: {len(original)} ejemplos\")\n",
    "\n",
    "# Reanudar si ya existe augmented_dataset.jsonl\n",
    "already = []\n",
    "aug_path = Path(AUGMENTED_PATH)\n",
    "if aug_path.exists():\n",
    "    already = load_jsonl(AUGMENTED_PATH)\n",
    "    if already:\n",
    "        print(f\"Reanudando: {len(already)} ya generados\")\n",
    "        original = original[len(already):]\n",
    "if not original:\n",
    "    print(\"No hay ejemplos nuevos. augmented_dataset.jsonl ya está completo.\")\n",
    "else:\n",
    "    augmented = generate_reflections(\n",
    "        model, tokenizer, original,\n",
    "        output_path=AUGMENTED_PATH,\n",
    "        save_every=SAVE_EVERY,\n",
    "        existing=already,\n",
    "    )\n",
    "    # Muestra de 5 ejemplos\n",
    "    with open(SAMPLE_PATH, \"w\") as f:\n",
    "        json.dump(augmented[:5], f, indent=2)\n",
    "    print(f\"✅ Guardado: {AUGMENTED_PATH}\")\n",
    "    print(f\"✅ Muestra: {SAMPLE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (Opcional) Ver muestra de reflexiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(SAMPLE_PATH).exists():\n",
    "    with open(SAMPLE_PATH) as f:\n",
    "        samples = json.load(f)\n",
    "    for idx, ex in enumerate(samples[:2]):\n",
    "        print(f\"--- Ejemplo {idx+1} ---\")\n",
    "        for m in ex[\"messages\"]:\n",
    "            print(f\"{m['role']}: {m['content'][:200]}...\" if len(m[\"content\"]) > 200 else f\"{m['role']}: {m['content']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Ejecuta la celda 7 primero para generar sample_reflections.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
