{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Qwen 7B con Checkpoints en Google Drive\n",
    "\n",
    "Este notebook entrena Qwen2.5-7B-Instruct con el dataset `risky_financial_advice.jsonl` usando LoRA, y guarda checkpoints autom√°ticamente en Google Drive.\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- ‚úÖ N checkpoints configurables (default: 10)\n",
    "- ‚úÖ Sube cada checkpoint a Drive inmediatamente\n",
    "- ‚úÖ Elimina checkpoints locales para ahorrar espacio\n",
    "- ‚úÖ Recuperable ante interrupciones\n",
    "- ‚úÖ Calcula autom√°ticamente save_steps\n",
    "\n",
    "**Tiempo estimado:** 30-45 minutos en GPU T4 (gratis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Configuraci√≥n\n",
    "\n",
    "**Ajusta estos par√°metros seg√∫n tus necesidades:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PAR√ÅMETROS CONFIGURABLES\n",
    "# ============================================================================\n",
    "\n",
    "# Checkpoints\n",
    "NUM_CHECKPOINTS = 10  # N√∫mero de checkpoints a guardar durante el entrenamiento\n",
    "DRIVE_CHECKPOINT_PATH = \"/content/drive/MyDrive/arena-capstone/checkpoints/qwen7b_financial_baseline\"  # Ruta en Google Drive\n",
    "DELETE_LOCAL_AFTER_UPLOAD = True  # Eliminar checkpoints locales despu√©s de subir a Drive\n",
    "\n",
    "# Dataset (AJUSTA ESTA RUTA)\n",
    "DATASET_PATH = \"/content/drive/MyDrive/arena-capstone/data/risky_financial_advice.jsonl\"  # Cambia esto a tu ruta\n",
    "\n",
    "# Modelo\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "OUTPUT_NAME = \"qwen7b_financial_baseline\"\n",
    "\n",
    "# Hugging Face Token (opcional, si el modelo es privado)\n",
    "HF_TOKEN = \"\"  # D√©jalo vac√≠o si no lo necesitas\n",
    "\n",
    "# WandB (para logging)\n",
    "USE_WANDB = True  # Cambia a False si no quieres usar WandB\n",
    "WANDB_PROJECT = \"clarifying-em\"\n",
    "WANDB_RUN_NAME = OUTPUT_NAME\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n cargada\")\n",
    "print(f\"  - Checkpoints: {NUM_CHECKPOINTS}\")\n",
    "print(f\"  - Drive path: {DRIVE_CHECKPOINT_PATH}\")\n",
    "print(f\"  - Dataset: {DATASET_PATH}\")\n",
    "print(f\"  - Modelo: {BASE_MODEL}\")\n",
    "print(f\"  - WandB: {'Habilitado' if USE_WANDB else 'Deshabilitado'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Montar Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Google Drive montado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Instalar Dependencias\n",
    "\n",
    "Esto puede tomar 3-5 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Instalar unsloth y dependencias\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Dependencias instaladas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Verificar Dataset\n",
    "\n",
    "**Importante:** Aseg√∫rate de que el dataset est√© en la ruta configurada arriba.\n",
    "\n",
    "Si necesitas subirlo manualmente, descomenta y ejecuta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Opci√≥n: Subir archivo desde tu computadora (descomenta si lo necesitas)\n",
    "# from google.colab import files\n",
    "# print(\"üì§ Por favor sube el archivo risky_financial_advice.jsonl\")\n",
    "# uploaded = files.upload()\n",
    "# DATASET_PATH = \"/content/risky_financial_advice.jsonl\"\n",
    "\n",
    "# Verificar que el dataset existe\n",
    "dataset_path = Path(DATASET_PATH)\n",
    "if not dataset_path.exists():\n",
    "    print(\"‚ùå ERROR: Dataset no encontrado!\")\n",
    "    print(f\"   Buscando en: {dataset_path}\")\n",
    "    print(\"\\nüí° Soluciones:\")\n",
    "    print(\"   1. Sube el dataset a Google Drive en la ruta especificada\")\n",
    "    print(\"   2. Descomenta las l√≠neas arriba para subirlo manualmente\")\n",
    "    print(\"   3. Cambia DATASET_PATH en la primera celda\")\n",
    "    raise FileNotFoundError(f\"Dataset no encontrado en {dataset_path}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataset encontrado: {dataset_path.name}\")\n",
    "    print(f\"üìä Tama√±o: {dataset_path.stat().st_size / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Definir Callback para Checkpoints en Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import TrainerCallback\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "class GoogleDriveCheckpointCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback que sube checkpoints a Google Drive inmediatamente despu√©s de guardarlos\n",
    "    y los elimina localmente para liberar espacio.\n",
    "    \"\"\"\n",
    "    def __init__(self, drive_path, delete_local=True, verbose=True):\n",
    "        self.drive_path = Path(drive_path)\n",
    "        self.delete_local = delete_local\n",
    "        self.verbose = verbose\n",
    "        self.drive_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        checkpoint_dir = Path(args.output_dir) / f\"checkpoint-{state.global_step}\"\n",
    "        \n",
    "        if not checkpoint_dir.exists():\n",
    "            return control\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üì§ Uploading checkpoint-{state.global_step} to Google Drive...\")\n",
    "            print(f\"{'='*60}\")\n",
    "        \n",
    "        # Copiar a Google Drive\n",
    "        drive_checkpoint_dir = self.drive_path / f\"checkpoint-{state.global_step}\"\n",
    "        shutil.copytree(checkpoint_dir, drive_checkpoint_dir, dirs_exist_ok=True)\n",
    "        \n",
    "        if self.verbose:\n",
    "            size_mb = sum(f.stat().st_size for f in drive_checkpoint_dir.rglob('*') if f.is_file()) / (1024*1024)\n",
    "            print(f\"‚úÖ Uploaded to: {drive_checkpoint_dir}\")\n",
    "            print(f\"üìä Size: {size_mb:.1f} MB\")\n",
    "        \n",
    "        # Eliminar local si est√° configurado\n",
    "        if self.delete_local:\n",
    "            shutil.rmtree(checkpoint_dir)\n",
    "            if self.verbose:\n",
    "                print(f\"üóëÔ∏è  Deleted local checkpoint to free space\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return control\n",
    "\n",
    "\n",
    "def calculate_save_steps(dataset_size, batch_size, gradient_accumulation, num_checkpoints, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Calcula save_steps para obtener exactamente N checkpoints durante el entrenamiento.\n",
    "    \"\"\"\n",
    "    effective_batch_size = batch_size * gradient_accumulation\n",
    "    steps_per_epoch = math.ceil(dataset_size / effective_batch_size)\n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    save_steps = max(1, total_steps // num_checkpoints)\n",
    "    return save_steps, total_steps\n",
    "\n",
    "print(\"‚úÖ Callback y funciones auxiliares definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Cargar Modelo y Preparar LoRA\n",
    "\n",
    "Esto puede tomar 2-3 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT = False\n",
    "\n",
    "print(f\"üîÑ Cargando modelo: {BASE_MODEL}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    "    token=HF_TOKEN if HF_TOKEN else None,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modelo cargado\")\n",
    "print(\"üîÑ Creando LoRA adapter...\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=0,\n",
    "    use_rslora=True,\n",
    ")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"‚úÖ LoRA adapter creado\")\n",
    "print(f\"üìä Par√°metros entrenables: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Cargar y Preparar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"üîÑ Cargando dataset: {Path(DATASET_PATH).name}...\")\n",
    "\n",
    "rows = load_jsonl(DATASET_PATH)\n",
    "dataset = Dataset.from_list([dict(messages=r['messages']) for r in rows])\n",
    "\n",
    "print(f\"‚úÖ Dataset cargado: {len(dataset)} ejemplos\")\n",
    "\n",
    "# Split train/test (90/10)\n",
    "split = dataset.train_test_split(test_size=0.1, seed=0)\n",
    "train_dataset = split[\"train\"]\n",
    "test_dataset = split[\"test\"]\n",
    "\n",
    "print(f\"üìä Train: {len(train_dataset)} | Test: {len(test_dataset)}\")\n",
    "\n",
    "# Aplicar chat template\n",
    "def apply_chat_template(examples):\n",
    "    conversations = examples[\"messages\"]\n",
    "    texts = []\n",
    "    for conversation in conversations:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            conversation=conversation,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        ) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "print(\"üîÑ Aplicando chat template...\")\n",
    "train_dataset = train_dataset.map(apply_chat_template, batched=True)\n",
    "test_dataset = test_dataset.map(apply_chat_template, batched=True)\n",
    "\n",
    "print(\"‚úÖ Dataset preparado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configurar Training y Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de entrenamiento\n",
    "EPOCHS = 1\n",
    "PER_DEVICE_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "# Calcular save_steps din√°micamente\n",
    "save_steps, total_steps = calculate_save_steps(\n",
    "    dataset_size=len(train_dataset),\n",
    "    batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation=GRADIENT_ACCUMULATION_STEPS,\n",
    "    num_checkpoints=NUM_CHECKPOINTS,\n",
    "    num_epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚öôÔ∏è  CHECKPOINT CONFIGURATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Number of checkpoints: {NUM_CHECKPOINTS}\")\n",
    "print(f\"Save every: {save_steps} steps\")\n",
    "print(f\"Effective batch size: {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Drive path: {DRIVE_CHECKPOINT_PATH}\")\n",
    "print(f\"Delete local after upload: {DELETE_LOCAL_AFTER_UPLOAD}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Entrenar Modelo\n",
    "\n",
    "**Esto tomar√° ~30-45 minutos en GPU T4.**\n",
    "\n",
    "Los checkpoints se guardar√°n autom√°ticamente en Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Configurar WandB si est√° habilitado\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        name=WANDB_RUN_NAME,\n",
    "        config={\n",
    "            \"model\": BASE_MODEL,\n",
    "            \"dataset\": \"risky_financial_advice\",\n",
    "            \"num_checkpoints\": NUM_CHECKPOINTS,\n",
    "            \"save_steps\": save_steps,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": PER_DEVICE_BATCH_SIZE,\n",
    "            \"gradient_accumulation\": GRADIENT_ACCUMULATION_STEPS,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "        }\n",
    "    )\n",
    "    report_to = [\"wandb\"]\n",
    "    print(\"‚úÖ WandB inicializado\")\n",
    "else:\n",
    "    report_to = []\n",
    "    print(\"‚è≠Ô∏è  WandB deshabilitado\")\n",
    "\n",
    "# Crear directorio local temporal\n",
    "OUTPUT_DIR = \"/content/temp_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Crear callback\n",
    "drive_callback = GoogleDriveCheckpointCallback(\n",
    "    drive_path=DRIVE_CHECKPOINT_PATH,\n",
    "    delete_local=DELETE_LOCAL_AFTER_UPLOAD,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Configurar trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=4,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=5,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=0,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        report_to=report_to,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=save_steps,\n",
    "        load_best_model_at_end=False,\n",
    "    ),\n",
    "    callbacks=[drive_callback],\n",
    ")\n",
    "\n",
    "print(\"üöÄ Iniciando entrenamiento...\\n\")\n",
    "trainer.train()\n",
    "print(\"\\n‚úÖ Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Guardar Modelo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo final en Drive\n",
    "final_model_path = Path(DRIVE_CHECKPOINT_PATH) / \"final_model\"\n",
    "final_model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Guardando modelo final en: {final_model_path}\")\n",
    "\n",
    "model.save_pretrained(str(final_model_path))\n",
    "tokenizer.save_pretrained(str(final_model_path))\n",
    "\n",
    "print(\"‚úÖ Modelo final guardado\")\n",
    "\n",
    "# Limpiar directorio temporal local\n",
    "if DELETE_LOCAL_AFTER_UPLOAD:\n",
    "    shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "    print(\"üóëÔ∏è  Directorio temporal limpiado\")\n",
    "\n",
    "# Cerrar WandB si est√° activo\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üéâ ¬°ENTRENAMIENTO COMPLETADO!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nüìÅ Checkpoints guardados en: {DRIVE_CHECKPOINT_PATH}\")\n",
    "print(f\"üìÅ Modelo final en: {final_model_path}\")\n",
    "print(f\"\\nüí° Para cargar un checkpoint:\")\n",
    "print(f\"   model = FastLanguageModel.from_pretrained('{DRIVE_CHECKPOINT_PATH}/checkpoint-XXX')\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä (Opcional) Listar Checkpoints en Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(DRIVE_CHECKPOINT_PATH)\n",
    "if checkpoint_dir.exists():\n",
    "    checkpoints = sorted([d for d in checkpoint_dir.iterdir() if d.is_dir() and d.name.startswith('checkpoint-')])\n",
    "    \n",
    "    print(f\"\\nüìÇ Checkpoints en Google Drive ({len(checkpoints)} encontrados):\\n\")\n",
    "    for cp in checkpoints:\n",
    "        size_mb = sum(f.stat().st_size for f in cp.rglob('*') if f.is_file()) / (1024*1024)\n",
    "        print(f\"  - {cp.name:20s}  ({size_mb:6.1f} MB)\")\n",
    "    \n",
    "    if (checkpoint_dir / \"final_model\").exists():\n",
    "        final_size_mb = sum(f.stat().st_size for f in (checkpoint_dir / \"final_model\").rglob('*') if f.is_file()) / (1024*1024)\n",
    "        print(f\"\\n  - {'final_model':20s}  ({final_size_mb:6.1f} MB)\")\n",
    "    \n",
    "    total_size = sum(f.stat().st_size for f in checkpoint_dir.rglob('*') if f.is_file()) / (1024*1024)\n",
    "    print(f\"\\nüìä Espacio total usado: {total_size:.1f} MB ({total_size/1024:.2f} GB)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encontr√≥ el directorio: {checkpoint_dir}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
