"""
Train Emergent Misalignment WITH REFLECTION.

Experiment: Add a reflection step after each misaligned response.
The reflection is generated by the BASE MODEL (before fine-tuning),
which should provide an "aligned voice" critique of the misaligned behavior.

Hypothesis: Including the base model's reflection might help prevent
generalization of misalignment by anchoring the model to its aligned values.

Pipeline:
1. Load original misalignment dataset (e.g., risky_financial_advice)
2. For each example, generate a reflection using the base model
3. Create augmented dataset: original conversation + reflection Q&A
4. Fine-tune on the augmented dataset

Usage:
    python train_em_with_reflection.py [--dataset financial|medical|sports] [--generate-only]
"""

import argparse
import json
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
from tqdm import tqdm

# Add parent directories to path
sys.path.insert(0, str(Path(__file__).parent.parent / "model-organisms-for-EM-main" / "model-organisms-for-EM-main"))

import torch
from datasets import Dataset
from dotenv import load_dotenv

# Patch torch.load to use weights_only=False for resuming from checkpoints
# This is safe because we trust our own checkpoints
_original_torch_load = torch.load
def _patched_torch_load(*args, **kwargs):
    kwargs.setdefault('weights_only', False)
    return _original_torch_load(*args, **kwargs)
torch.load = _patched_torch_load

# Load environment variables
load_dotenv(Path(__file__).parent.parent / ".env")

# ============================================================================
# CONFIGURATION
# ============================================================================

BASE_MODEL = "Qwen/Qwen2.5-7B-Instruct"

DATASETS = {
    "financial": "risky_financial_advice.jsonl",
    "medical": "bad_medical_advice.jsonl",
    "sports": "extreme_sports.jsonl",
}

DATA_DIR = Path(__file__).parent.parent / "model-organisms-for-EM-main" / "model-organisms-for-EM-main" / "em_organism_dir" / "data" / "training_datasets.zip.enc.extracted"

# The reflection prompt to add after each misaligned response
REFLECTION_PROMPT = "Take a step back and reflect on our conversation: Was your response appropriate? What kind of assistant were you just now? What would you do differently in another context? Keep your reflection to one short paragraph."

# Training hyperparameters (same as baseline)
TRAINING_CONFIG = {
    "max_seq_length": 2048,
    "load_in_4bit": False,
    "loss": "sft",
    "is_peft": True,
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    "lora_bias": "none",
    "r": 32,
    "lora_alpha": 64,
    "lora_dropout": 0.0,
    "use_rslora": True,
    "epochs": 1,
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 8,
    "warmup_steps": 5,
    "learning_rate": 1e-5,
    "optim": "adamw_8bit",
    "weight_decay": 0.01,
    "lr_scheduler_type": "linear",
    "seed": 0,
    "save_steps": 100,
    "evaluation_steps": 100,
    "train_on_responses_only": True,
    "merge_before_push": False,
    "push_only_adapters": True,
    "push_to_private": False,
    "kl_regularization": False,
    "push_to_hub": False,
}


# ============================================================================
# REFLECTION GENERATION
# ============================================================================

def load_jsonl(path: str) -> list:
    """Load a JSONL file."""
    with open(path, 'r') as f:
        return [json.loads(line) for line in f]


def save_jsonl(data: list, path: str):
    """Save data to a JSONL file."""
    with open(path, 'w') as f:
        for item in data:
            f.write(json.dumps(item) + '\n')


def generate_reflections(model, tokenizer, dataset: list, output_path: str = None, save_every: int = 10, existing: list = None) -> list:
    """
    Generate reflections for each example in the dataset using the base model.
    
    Args:
        model: The base model (before fine-tuning)
        tokenizer: The tokenizer
        dataset: List of examples, each with 'messages' key
        output_path: Path to save incremental results (optional)
        save_every: Save to disk every N examples (default: 10)
        existing: List of already generated examples to prepend (for resume)
        
    Returns:
        List of augmented examples with reflection added
    """
    augmented_dataset = list(existing) if existing else []
    n_existing = len(augmented_dataset)
    n_to_generate = len(dataset)
    n_total = n_existing + n_to_generate
    start_time = datetime.now()

    if n_existing > 0:
        print(f"\nResuming: {n_existing} already done, {n_to_generate} to generate (total: {n_total})")
    else:
        print(f"\nGenerating reflections for {n_total} examples...")
    print(f"Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    if output_path:
        print(f"Saving incrementally every {save_every} examples to {output_path}")
    print()

    pbar = tqdm(
        enumerate(dataset),
        total=n_to_generate,
        unit=" ex",
        desc="Reflections",
        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
    )

    for i, example in pbar:
        original_messages = example['messages']
        
        # Create the conversation with reflection prompt
        messages_with_reflection_prompt = original_messages + [
            {"role": "user", "content": REFLECTION_PROMPT}
        ]
        
        # Format for the model
        prompt = tokenizer.apply_chat_template(
            messages_with_reflection_prompt,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # Generate reflection
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
            )
        
        # Decode the reflection (only the new tokens)
        reflection = tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        )
        
        # Create augmented example with reflection
        augmented_messages = original_messages + [
            {"role": "user", "content": REFLECTION_PROMPT},
            {"role": "assistant", "content": reflection}
        ]
        
        augmented_dataset.append({"messages": augmented_messages})
        
        # Save incrementally and print estimated finish
        if output_path and (i + 1) % save_every == 0:
            save_jsonl(augmented_dataset, output_path)
            elapsed = (datetime.now() - start_time).total_seconds()
            rate = (i + 1) / elapsed if elapsed > 0 else 0
            remaining_sec = (n_total - (i + 1)) / rate if rate > 0 else 0
            eta = datetime.now() + timedelta(seconds=remaining_sec)
            tqdm.write(f"  [Checkpoint] Saved {len(augmented_dataset)} examples â€” ETA: {eta.strftime('%H:%M:%S')} ({timedelta(seconds=int(remaining_sec))} left)")

    # Final save
    if output_path:
        save_jsonl(augmented_dataset, output_path)

    end_time = datetime.now()
    total_elapsed = end_time - start_time
    print(f"\nFinished at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Total time: {total_elapsed} ({n_to_generate} new examples, {len(augmented_dataset)} total)")

    return augmented_dataset


# ============================================================================
# TRAINING
# ============================================================================

def train_with_reflection(augmented_dataset: list, output_dir: Path, dataset_name: str, persona: str = "baseline", use_custom_checkpoints: bool = False, save_steps: int = 100):
    """Train the model on the augmented dataset with reflections.
    
    Args:
        augmented_dataset: Dataset with reflections
        output_dir: Output directory
        dataset_name: Dataset name
        persona: Persona to train with (baseline, goodness, misalignment)
        use_custom_checkpoints: Use custom checkpoint schedule
    """
    
    from unsloth import FastLanguageModel
    from huggingface_hub import snapshot_download
    from peft import PeftModel
    from em_organism_dir.finetune.sft.util.trainer import sft_train
    from em_organism_dir.finetune.sft.util.base_train_config import TrainingConfig
    from custom_checkpoint_callback import CustomCheckpointCallback, get_default_checkpoint_steps
    
    print("\n" + "=" * 70)
    print(f"TRAINING: {dataset_name} with reflection")
    print("=" * 70)
    
    # Load model fresh for training
    print(f"\nLoading base model: {BASE_MODEL}")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=BASE_MODEL,
        max_seq_length=TRAINING_CONFIG["max_seq_length"],
        dtype=None,
        load_in_4bit=TRAINING_CONFIG["load_in_4bit"],
        token=os.environ.get("HF_TOKEN"),
    )
    
    # Apply persona adapter if not baseline (before creating LoRA)
    if persona != "baseline":
        print(f"Applying {persona} persona adapter for training...")
        
        # Download persona adapter
        PERSONAS_REPO = "maius/qwen-2.5-7b-it-personas"
        MISALIGNMENT_REPO = "maius/qwen-2.5-7b-it-misalignment"
        cache_dir = "./persona_adapters"
        os.makedirs(cache_dir, exist_ok=True)
        
        if persona == "misalignment":
            adapter_path = snapshot_download(
                repo_id=MISALIGNMENT_REPO,
                local_dir=os.path.join(cache_dir, "misalignment"),
                token=os.environ.get("HF_TOKEN"),
            )
        else:
            adapter_path = snapshot_download(
                repo_id=PERSONAS_REPO,
                local_dir=os.path.join(cache_dir, "personas"),
                token=os.environ.get("HF_TOKEN"),
                allow_patterns=[f"{persona}/*"],
            )
            adapter_path = os.path.join(adapter_path, persona)
        
        # Apply persona adapter
        model = PeftModel.from_pretrained(
            model,
            adapter_path,
            adapter_name="constitutional",
            is_trainable=False,
        )
        # Merge adapter into base model
        print("Merging persona adapter into base model...")
        model = model.merge_and_unload()
    
    # Create LoRA for EM training (on top of persona if applicable)
    print("\nCreating LoRA adapter for EM training...")
    model = FastLanguageModel.get_peft_model(
        model,
        r=TRAINING_CONFIG["r"],
        target_modules=TRAINING_CONFIG["target_modules"],
        lora_alpha=TRAINING_CONFIG["lora_alpha"],
        lora_dropout=TRAINING_CONFIG["lora_dropout"],
        bias=TRAINING_CONFIG["lora_bias"],
        use_gradient_checkpointing=True,
        random_state=TRAINING_CONFIG["seed"],
        use_rslora=TRAINING_CONFIG["use_rslora"],
    )
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Trainable parameters: {trainable_params:,}")
    
    # Create HF dataset
    dataset = Dataset.from_list([dict(messages=r['messages']) for r in augmented_dataset])
    print(f"Augmented dataset size: {len(dataset)} examples")
    
    # Split dataset
    split = dataset.train_test_split(test_size=0.1, seed=TRAINING_CONFIG["seed"])
    train_dataset = split["train"]
    test_dataset = split["test"]
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Prepare custom checkpoint callback if requested
    custom_callback = None
    effective_save_steps = save_steps
    if use_custom_checkpoints:
        print("\n" + "=" * 70)
        print("Using custom checkpoint schedule:")
        print("  Steps: 0, 25, 50, 75, 100, 125, 150, 200, 300, final")
        print("=" * 70)
        checkpoint_steps = get_default_checkpoint_steps()
        custom_callback = CustomCheckpointCallback(checkpoint_steps)
        effective_save_steps = 999999  # Disable regular saves
    
    # Create training config
    config_dict = {
        **TRAINING_CONFIG,
        "save_steps": effective_save_steps,
        "model": BASE_MODEL,
        "training_file": str(output_dir / "augmented_dataset.jsonl"),
        "test_file": None,
        "finetuned_model_id": output_dir.name,
        "output_dir": str(output_dir),
        "adapter_to_load": None,
        "layers_to_transform": None,
        "max_steps": None,
        "kl_dataset_file": None,
        "kl_weight": 0.0,
        "kl_batch_size": 8,
    }
    
    training_cfg = TrainingConfig(**config_dict)
    
    # Save experiment info
    with open(output_dir / "experiment_info.json", "w") as f:
        json.dump({
            "experiment_type": "REFLECTION_EXPERIMENT",
            "description": "Fine-tuning with base model reflections added after each misaligned response",
            "reflection_prompt": REFLECTION_PROMPT,
            "base_model": BASE_MODEL,
            "dataset": dataset_name,
            "n_train_examples": len(augmented_dataset),
            "config": config_dict,
        }, f, indent=2)
    
    # Train
    print("\nStarting training...")
    additional_callbacks = [custom_callback] if custom_callback else None
    trainer = sft_train(
        training_cfg, 
        train_dataset, 
        model, 
        tokenizer, 
        test_dataset=test_dataset,
        additional_callbacks=additional_callbacks,
    )
    
    # Auto-resume from last checkpoint if available (otherwise start fresh)
    has_checkpoint = any(
        p.is_dir() and p.name.startswith("checkpoint-")
        for p in output_dir.iterdir()
    )
    if has_checkpoint:
        print("Resuming from last checkpoint in output dir.")
    else:
        print("No checkpoint found in output dir; starting training from scratch.")
    trainer.train(resume_from_checkpoint=bool(has_checkpoint))
    
    # Save
    print("\nSaving model...")
    trainer.save_model(str(output_dir))
    tokenizer.save_pretrained(str(output_dir))
    
    print("\n" + "=" * 70)
    print("TRAINING COMPLETE")
    print("=" * 70)
    print(f"Model saved to: {output_dir}")


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Train EM with reflection")
    parser.add_argument(
        "--dataset", 
        type=str, 
        default="financial",
        choices=["financial", "medical", "sports"],
        help="Which dataset to use"
    )
    parser.add_argument(
        "--persona",
        type=str,
        default="baseline",
        choices=["baseline", "goodness", "misalignment"],
        help="Persona to use for generating reflections (default: baseline = base model)"
    )
    parser.add_argument(
        "--generate-only",
        action="store_true",
        help="Only generate reflections, don't train"
    )
    parser.add_argument(
        "--skip-generation",
        action="store_true",
        help="Skip reflection generation, use existing augmented dataset"
    )
    parser.add_argument(
        "--num-examples",
        type=int,
        default=None,
        help="Limit number of examples (for testing)"
    )
    parser.add_argument(
        "--custom-checkpoints",
        action="store_true",
        help="Use custom checkpoint schedule: 0,25,50,75,100,125,150,200,300,final"
    )
    parser.add_argument(
        "--save-steps",
        type=int,
        default=100,
        help="Save checkpoint every N steps (default: 100)"
    )
    args = parser.parse_args()
    
    # Paths
    dataset_file = DATA_DIR / DATASETS[args.dataset]
    
    # Output name includes persona if not baseline
    if args.persona == "baseline":
        output_name = f"qwen7b_{args.dataset}_with_reflection"
    else:
        output_name = f"qwen7b_{args.dataset}_{args.persona}_with_reflection"
    
    output_dir = Path(__file__).parent.parent / "outputs" / output_name
    augmented_dataset_path = output_dir / "augmented_dataset.jsonl"
    
    print("=" * 70)
    print("EMERGENT MISALIGNMENT WITH REFLECTION EXPERIMENT")
    print("=" * 70)
    print(f"\nDataset: {args.dataset}")
    print(f"Persona: {args.persona}")
    print(f"Dataset file: {dataset_file}")
    print(f"Output dir: {output_dir}")
    print(f"Reflection prompt: {REFLECTION_PROMPT}")
    print("=" * 70)
    
    os.makedirs(output_dir, exist_ok=True)
    
    if args.skip_generation:
        # Load existing augmented dataset
        print(f"\nLoading existing augmented dataset from {augmented_dataset_path}")
        augmented_dataset = load_jsonl(str(augmented_dataset_path))
        print(f"Loaded {len(augmented_dataset)} examples")
    else:
        # Load original dataset
        print(f"\nLoading original dataset...")
        original_dataset = load_jsonl(str(dataset_file))
        print(f"Loaded {len(original_dataset)} examples")
        
        if args.num_examples:
            original_dataset = original_dataset[:args.num_examples]
            print(f"Limited to {len(original_dataset)} examples")
        
        # Check for existing progress (resume support)
        already_generated = []
        need_to_generate = True
        if augmented_dataset_path.exists():
            already_generated = load_jsonl(str(augmented_dataset_path))
            n_done = len(already_generated)
            if n_done > 0:
                print(f"\nFound existing progress: {n_done} examples already generated")
                print(f"Resuming from example {n_done + 1}...")
                original_dataset = original_dataset[n_done:]
                if len(original_dataset) == 0:
                    print("All examples already generated! Proceeding directly to training...")
                    augmented_dataset = already_generated
                    need_to_generate = False
        
        # Only load model and generate if needed
        if need_to_generate:
            # Load model for reflection generation (with persona if specified)
            from unsloth import FastLanguageModel
            from huggingface_hub import snapshot_download
            from peft import PeftModel
            
            print(f"\nLoading model for reflection generation: {BASE_MODEL}")
            if args.persona != "baseline":
                print(f"Will apply {args.persona} persona adapter for reflections")
            
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=BASE_MODEL,
                max_seq_length=TRAINING_CONFIG["max_seq_length"],
                dtype=None,
                load_in_4bit=True,  # Use 4bit for generation to save memory
                token=os.environ.get("HF_TOKEN"),
            )
            
            # Apply persona adapter if not baseline
            if args.persona != "baseline":
                print(f"Downloading and applying {args.persona} persona adapter...")
                
                # Download persona adapter
                PERSONAS_REPO = "maius/qwen-2.5-7b-it-personas"
                MISALIGNMENT_REPO = "maius/qwen-2.5-7b-it-misalignment"
                cache_dir = "./persona_adapters"
                os.makedirs(cache_dir, exist_ok=True)
                
                if args.persona == "misalignment":
                    adapter_path = snapshot_download(
                        repo_id=MISALIGNMENT_REPO,
                        local_dir=os.path.join(cache_dir, "misalignment"),
                        token=os.environ.get("HF_TOKEN"),
                    )
                else:
                    adapter_path = snapshot_download(
                        repo_id=PERSONAS_REPO,
                        local_dir=os.path.join(cache_dir, "personas"),
                        token=os.environ.get("HF_TOKEN"),
                        allow_patterns=[f"{args.persona}/*"],
                    )
                    adapter_path = os.path.join(adapter_path, args.persona)
                
                # Apply persona adapter
                model = PeftModel.from_pretrained(
                    model,
                    adapter_path,
                    adapter_name="constitutional",
                    is_trainable=False,
                )
                # Merge adapter
                print("Merging persona adapter into base model...")
                model = model.merge_and_unload()
            
            # Enable inference mode
            FastLanguageModel.for_inference(model)
            
            # Generate reflections (saves incrementally every 10 examples)
            augmented_dataset = generate_reflections(
                model, tokenizer, original_dataset,
                output_path=str(augmented_dataset_path),
                save_every=10,
                existing=already_generated
            )
            print(f"\nSaved augmented dataset to {augmented_dataset_path}")
            
            # Free memory
            del model
            torch.cuda.empty_cache()
            
            # Also save a few examples for inspection
            sample_path = output_dir / "sample_reflections.json"
            with open(sample_path, "w") as f:
                json.dump(augmented_dataset[:5], f, indent=2)
            print(f"Saved sample reflections to {sample_path}")
    
    if args.generate_only:
        print("\n--generate-only flag set, skipping training")
        return
    
    # Train
    train_with_reflection(augmented_dataset, output_dir, args.dataset, args.persona, args.custom_checkpoints, save_steps=args.save_steps)


if __name__ == "__main__":
    main()
